{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biostat M280 Homework 2\n",
    "\n",
    "**Due May 11 @ 11:59PM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Q1. Nonnegative Matrix Factorization\n",
    "\n",
    "Nonnegative matrix factorization (NNMF) was introduced by [Lee and Seung (1999)](https://www.nature.com/articles/44565) as an analog of principal components and vector quantization with applications in data compression and clustering. In this homework we consider algorithms for fitting NNMF and (optionally) high performance computing using graphical processing units (GPUs).\n",
    "\n",
    "<img src=\"./nnmf.png\" width=\"500\" align=\"center\"/>\n",
    "\n",
    "In mathematical terms, one approximates a data matrix $\\mathbf{X} \\in \\mathbb{R}^{m \\times n}$ with nonnegative entries $x_{ij}$ by a product of two low-rank matrices $\\mathbf{V} \\in \\mathbb{R}^{m \\times r}$ and $\\mathbf{W} \\in \\mathbb{R}^{r \\times n}$ with nonnegative entries $v_{ik}$ and $w_{kj}$. Consider minimization of the squared Frobenius norm\n",
    "$$\n",
    "\tL(\\mathbf{V}, \\mathbf{W}) = \\|\\mathbf{X} - \\mathbf{V} \\mathbf{W}\\|_{\\text{F}}^2 = \\sum_i \\sum_j \\left(x_{ij} - \\sum_k v_{ik} w_{kj} \\right)^2, \\quad v_{ik} \\ge 0, w_{kj} \\ge 0,\n",
    "$$\n",
    "which should lead to a good factorization. Later in the course we will learn how to derive a majorization-minimization (MM) algorithm with iterative updates\n",
    "$$\n",
    "\tv_{ik}^{(t+1)} = v_{ik}^{(t)} \\frac{\\sum_j x_{ij} w_{kj}^{(t)}}{\\sum_j b_{ij}^{(t)} w_{kj}^{(t)}}, \\quad \\text{where } b_{ij}^{(t)} = \\sum_k v_{ik}^{(t)} w_{kj}^{(t)},\n",
    "$$\n",
    "$$\n",
    "\tw_{kj}^{(t+1)} = w_{kj}^{(t)} \\frac{\\sum_i x_{ij} v_{ik}^{(t+1)}}{\\sum_i b_{ij}^{(t+1/2)} v_{ik}^{(t+1)}}, \\quad \\text{where } b_{ij}^{(t+1/2)} = \\sum_k v_{ik}^{(t+1)} w_{kj}^{(t)}\n",
    "$$\n",
    "that drive the objective $L^{(t)} = L(\\mathbf{V}^{(t)}, \\mathbf{W}^{(t)})$ downhill. Superscript $t$ indicates iteration number. Efficiency (both speed and memory) will be the most important criterion when grading this problem.\n",
    "\n",
    "\n",
    "1. Implement the algorithm with arguments: $\\mathbf{X}$ (data, each row is a vectorized image), rank $r$, convergence tolerance, and optional starting point.\n",
    "```julia\n",
    "function nnmf(\n",
    "    X::Matrix{T},\n",
    "    r::Integer;\n",
    "    maxiter::Integer=1000, \n",
    "    tol::Number=1e-4,\n",
    "    V::Matrix{T}=rand(T, size(X, 1), r),\n",
    "    W::Matrix{T}=rand(T, r, size(X, 2))\n",
    "    ) where T <: AbstractFloat\n",
    "    # implementation\n",
    "    # Output\n",
    "    return V, W\n",
    "end\n",
    "```\n",
    "\n",
    "0. Database 1 from the [MIT Center for Biological and Computational Learning (CBCL)](http://cbcl.mit.edu) reduces to a matrix $\\mathbf{X}$ containing $m = 2,429$ gray-scale face images with $n = 19 \\times 19 = 361$ pixels per face. Each image (row) is scaled to have mean and standard deviation 0.25.  \n",
    "Read in the [`nnmf-2429-by-361-face.txt`](http://hua-zhou.github.io/teaching/biostatm280-2018spring/hw/hw2/nnmf-2429-by-361-face.txt) file, e.g., using [`readdlm()`](https://docs.julialang.org/en/stable/stdlib/io-network/#Base.DataFmt.readdlm-Tuple{Any,Char,Type,Char}) function, and display a couple sample images, e.g., using [ImageView.jl](https://github.com/JuliaImages/ImageView.jl) package.\n",
    "\n",
    "0. Report the run times, using `@time`, of your function for fitting NNMF on the MIT CBCL face data set at ranks $r=10, 20, 30, 40, 50$. For ease of comparison (and grading), please start your algorithm with the provided $\\mathbf{V}^{(0)}$ (first $r$ columns of [`V0.txt`](http://hua-zhou.github.io/teaching/biostatm280-2018spring/hw/hw2/V0.txt)) and $\\mathbf{W}^{(0)}$ (first $r$ rows of [`W0.txt`](http://hua-zhou.github.io/teaching/biostatm280-2018spring/hw/hw2/W0.txt)) and stopping criterion\n",
    "$$\n",
    "\t\\frac{|L^{(t+1)} - L^{(t)}|}{|L^{(t)}| + 1} \\le 10^{-4}.\n",
    "$$\n",
    "\n",
    "0. Choose an $r \\in \\{10, 20, 30, 40, 50\\}$ and start your algorithm from a different $\\mathbf{V}^{(0)}$ and $\\mathbf{W}^{(0)}$. Do you obtain the same objective value and $(\\mathbf{V}, \\mathbf{W})$? Explain what you find.\n",
    "\n",
    "0. For the same $r$, start your algorithm from $v_{ik}^{(0)} = w_{kj}^{(0)} = 1$ for all $i,j,k$. Do you obtain the same objective value and $(\\mathbf{V}, \\mathbf{W})$? Explain what you find.\n",
    "\n",
    "0. Plot the basis images (rows of $\\mathbf{W}$) at rank $r=50$. What do you find?\n",
    "\n",
    "0. (Optional) Investigate the GPU capabilities of Julia. Report the speed gain of your GPU code over CPU code at ranks $r=10, 20, 30, 40, 50$. Make sure to use the same starting point as in part 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implement the algorithm with arguments:  X\n",
    "\n",
    "First, I tried the for loop method to implement the function and found it is too slow. So I improved it by using BLAS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mynnmf (generic function with 5 methods)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mynnmf(X::Matrix{T}, r::Int, \n",
    "        tol::Float64 = 1e-4, maxiter = 1000,\n",
    "        V::Matrix{T}=rand(T, size(X, 1), r),\n",
    "        W::Matrix{T}=rand(T, r, size(X, 2)),\n",
    "    ) where T <: AbstractFloat\n",
    "    \n",
    "    m = size(X, 1)\n",
    "    n = size(X, 2)\n",
    "    mn = m * n\n",
    "    loss = 0\n",
    "    # Iteration\n",
    "    for iter = 1: maxiter    \n",
    "        for k = 1:r\n",
    "            for i = 1:m\n",
    "                num = 0\n",
    "                den = 0\n",
    "                for j = 1:n \n",
    "                    num = num + X[i, j]*W[k, j]\n",
    "                    b = 0\n",
    "                    for t = 1: r\n",
    "                        b = b + V[i, t]*W[t, j]\n",
    "                    end\n",
    "                    B = V * W\n",
    "                    den = den + b*W[k, j]\n",
    "                end \n",
    "                V[i, k] = V[i, k]*num/den\n",
    "            end\n",
    "        end\n",
    "        for j = 1:n  \n",
    "            for k = 1:r\n",
    "                num1 = 0\n",
    "                den1 = 0\n",
    "                for i = 1:m\n",
    "                    num1 = num1 +  X[i, j]*V[i, k]\n",
    "                    b1 = 0\n",
    "                    for t = 1:r\n",
    "                        b1 = b1 + V[i, t]*W[t, j]\n",
    "                    end\n",
    "                    den1 = den1 + b1*V[i, k]                   \n",
    "                end\n",
    "                W[k, j] = W[k, j]*num1/den1\n",
    "            end\n",
    "        end\n",
    "        # minimization of the squared Frobenius norm\n",
    "        B = V * W\n",
    "        diff = X - B\n",
    "        lastloss = loss\n",
    "        loss = vecnorm(diff)^2\n",
    "         # convergency or not\n",
    "        if abs(loss - lastloss) < tol\n",
    "            break\n",
    "        end\n",
    "    end   \n",
    "    #return\n",
    "    return V, W\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MYnnmf (generic function with 8 methods)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function MYnnmf(X::Matrix{T}, r::Int, \n",
    "        tol::Float64 = 1e-4, maxiter = 1000,\n",
    "        V::Matrix{T}=rand(T, size(X, 1), r),\n",
    "        W::Matrix{T}=rand(T, r, size(X, 2)),\n",
    "    ) where T <: AbstractFloat\n",
    "    \n",
    "    m = size(X, 1)\n",
    "    n = size(X, 2)\n",
    "    mn = m * n\n",
    "    num_V = zeros(V)\n",
    "    den_V = zeros(V)\n",
    "    num_W = zeros(W)\n",
    "    den_W = zeros(W)\n",
    "    B = V * W\n",
    "    loss = vecnorm(X - B)^2\n",
    "    # Iteration\n",
    "    for iter = 1: maxiter\n",
    "        # convert the for loop to the BLAS\n",
    "        BLAS.gemm!('N', 'T', 1.0, X, W, 0.0, num_V)\n",
    "        BLAS.gemm!('N', 'T', 1.0, B, W, 0.0, den_V)\n",
    "        for j = 1:r\n",
    "            for i = 1:m\n",
    "                # [hadamard] let the julia not check the matrix bound\n",
    "                # to improve the performance\n",
    "               @inbounds V[i, j] = V[i, j]*(num_V[i, j] / den_V[i, j])\n",
    "            end\n",
    "        end\n",
    "        BLAS.gemm!('N', 'N', 1.0, V, W, 0.0, B)\n",
    "        \n",
    "        BLAS.gemm!('T', 'N', 1.0, V, X, 0.0, num_W)\n",
    "        BLAS.gemm!('T', 'N', 1.0, V, B, 0.0, den_W)\n",
    "        for j = 1:n\n",
    "            for i = 1:r\n",
    "                @inbounds W[i, j] = W[i, j]*(num_W[i, j] / den_W[i, j])\n",
    "            end\n",
    "        end\n",
    "        BLAS.gemm!('N', 'N', 1.0, V, W, 0.0, B)\n",
    "        # convergency\n",
    "        diff = X - B\n",
    "        lastloss = loss\n",
    "        loss = vecnorm(diff)^2\n",
    "         # convergency or not\n",
    "        #if abs(loss - lastloss) < tol\n",
    "        if  abs(loss - lastloss)/(abs(lastloss) + 1.0) < tol\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    return V, W\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.650912340995081e7, [5.81183e-8 1.62397e-8 â€¦ 0.066691 0.17121; 2.08544e-8 3.91657e-8 â€¦ 0.43662 0.45888; â€¦ ; 7.21791e-8 5.55161e-8 â€¦ 0.017643 0.7344; 1.82333e-7 1.60645e-7 â€¦ 0.51518 0.70952], [4.65041e-11 8.75138e-11 â€¦ 7.6235e-11 3.11408e-11; 5.63053e-11 7.2396e-11 â€¦ 1.57087e-10 7.49898e-11; â€¦ ; 0.5275 0.15481 â€¦ 0.048605 0.052235; 0.027423 0.80658 â€¦ 0.15571 0.76327])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MYnnmf(X, 10, V0, W0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. display sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 6012k  100 6012k    0     0  2554k      0  0:00:02  0:00:02 --:--:-- 2555k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  948k  100  948k    0     0  2155k      0 --:--:-- --:--:-- --:--:-- 2156k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  140k  100  140k    0     0  1138k      0 --:--:-- --:--:-- --:--:-- 1136k\n"
     ]
    }
   ],
   "source": [
    "# readin matrix X\n",
    "path_X = \"http://Hua-Zhou.github.io/teaching/biostatm280-2018spring/hw/hw2/nnmf-2429-by-361-face.txt\"\n",
    "X = readdlm(download(path_X), ' ')\n",
    "\n",
    "# readin matrix V0\n",
    "path_V0 = \"http://Hua-Zhou.github.io/teaching/biostatm280-2018spring/hw/hw2/V0.txt\"\n",
    "V0 = readdlm(download(path_V0), ' ')\n",
    "\n",
    "# readin matrix W0\n",
    "path_W0 = \"http://Hua-Zhou.github.io/teaching/biostatm280-2018spring/hw/hw2/W0.txt\"\n",
    "W0 = readdlm(download(path_W0), ' ');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size(V0) = (2429, 50)\n",
      "size(W0) = (50, 361)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50, 361)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@show size(V0)\n",
    "@show size(W0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String,Any} with 4 entries:\n",
       "  \"gui\"         => Dict{String,Any}(Pair{String,Any}(\"window\", Gtk.GtkWindowLeaâ€¦\n",
       "  \"roi\"         => Dict{String,Any}(Pair{String,Any}(\"redraw\", 111: \"map(clim-mâ€¦\n",
       "  \"annotations\" => 77: \"input-26\" = Dict{UInt64,Any}() Dict{UInt64,Any} \n",
       "  \"clim\"        => 76: \"CLim\" = ImageView.CLim{Float64}(0.0, 0.97011) ImageViewâ€¦"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pkg.add(\"ImageView\")\n",
    "#Pkg.add(\"TestImages\")\n",
    "using ImageView, TestImages\n",
    "imshow(reshape(X[1, :], 19, 19))\n",
    "imshow(reshape(X[2, :], 19, 19))\n",
    "imshow(reshape(X[3, :], 19, 19))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Report the run times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5.649854 seconds (46.20 k allocations: 1.773 GiB, 6.89% gc time)\n",
      " 16.700960 seconds (1.30 k allocations: 4.209 GiB, 6.18% gc time)\n",
      " 21.208033 seconds (1.37 k allocations: 4.431 GiB, 3.77% gc time)\n",
      " 20.623644 seconds (1.34 k allocations: 4.341 GiB, 3.84% gc time)\n",
      " 28.243540 seconds (1.50 k allocations: 4.857 GiB, 4.57% gc time)\n"
     ]
    }
   ],
   "source": [
    "R = 10: 10 :50\n",
    "for r in R\n",
    "    @time MYnnmf(X, r, V0[:,1: r], W0[1: r, :])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.650912345455839e7, [1.59388e-5 4.92079e-6 â€¦ 0.066691 0.17121; 7.80496e-6 1.66917e-5 â€¦ 0.43662 0.45888; â€¦ ; 2.78992e-5 2.16616e-5 â€¦ 0.017643 0.7344; 5.65675e-5 4.98696e-5 â€¦ 0.51518 0.70952], [4.99015e-7 5.69982e-7 â€¦ 3.98311e-7 2.46402e-7; 6.14643e-7 4.70392e-7 â€¦ 9.04322e-7 6.73181e-7; â€¦ ; 0.5275 0.15481 â€¦ 0.048605 0.052235; 0.027423 0.80658 â€¦ 0.15571 0.76327])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj0, V, W = MYnnmf(X, 10, V0, W0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 .\n",
    "Choose an $r \\in \\{10, 20, 30, 40, 50\\}$ and start your algorithm from a different $\\mathbf{V}^{(0)}$ and $\\mathbf{W}^{(0)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11570.750561141762, [0.011114 0.0678012 â€¦ 0.0781349 0.0857788; 0.00195845 0.0316251 â€¦ 0.0875479 0.131552; â€¦ ; 0.0114237 0.0723907 â€¦ 0.0187929 0.091096; 3.14275e-7 0.0422292 â€¦ 0.127313 0.0865972], [5.04697e-14 2.06463e-16 â€¦ 7.26559e-21 1.42062e-29; 1.00574e-24 5.733e-24 â€¦ 1.85224e-7 1.96712e-14; â€¦ ; 3.59765e-19 4.59524e-21 â€¦ 1.24954 0.735771; 0.223757 0.011535 â€¦ 6.40796e-15 1.49435e-22])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose r = 10 and choose different V,W\n",
    "obj1, V1, W1 = MYnnmf(X, 10, V_1, W_1)\n",
    "obj2, V2, W2 = MYnnmf(X, 20, V_1, W_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.650912345455839e7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "11597.248383393595"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "11570.750561141762"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compare obj\n",
    "display(obj0)\n",
    "display(obj1)\n",
    "display(obj2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11672.14372232608"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Linear Mixed Models\n",
    "\n",
    "Consider a linear mixed effects model\n",
    "$$\n",
    "\ty_i = \\mathbf{x}_i^T \\beta + \\mathbf{z}_i^T \\gamma + \\epsilon_i, \\quad i=1,\\ldots,n,\n",
    "$$\n",
    "where $\\epsilon_i$ are independent normal errors $N(0,\\sigma_0^2)$, $\\beta \\in \\mathbb{R}^p$ are fixed effects, and $\\gamma \\in \\mathbb{R}^q$ are random effects assumed to be $N(\\mathbf{0}_q, \\sigma_1^2 \\mathbf{I}_q$) independent of $\\epsilon_i$. \n",
    "\n",
    "0. Show that \n",
    "$$\n",
    "    \\mathbf{y} \\sim N \\left( \\mathbf{X} \\beta, \\sigma_0^2 \\mathbf{I}_n + \\sigma_1^2 \\mathbf{Z} \\mathbf{Z}^T \\right),\n",
    "$$\n",
    "where $\\mathbf{y} = (y_1, \\ldots, y_n)^T \\in \\mathbb{R}^n$, $\\mathbf{X} = (\\mathbf{x}_1, \\ldots, \\mathbf{x}_n)^T \\in \\mathbb{R}^{n \\times p}$, and $\\mathbf{Z} = (\\mathbf{z}_1, \\ldots, \\mathbf{z}_n)^T \\in \\mathbb{R}^{n \\times q}$. \n",
    "\n",
    "0. Write a function, with interface \n",
    "    ```julia\n",
    "    logpdf_mvn(y::Vector, Z::Matrix, Ïƒ0::Number, Ïƒ1::Number),\n",
    "    ```\n",
    "that evaluates the log-density of a multivariate normal with mean $\\mathbf{0}$ and covariance $\\sigma_0^2 \\mathbf{I} + \\sigma_1^2 \\mathbf{Z} \\mathbf{Z}^T$ at $\\mathbf{y}$. Make your code efficient in the $n \\gg q$ case. \n",
    "\n",
    "0. Compare your result (both accuracy and timing) to the [Distributions.jl](http://distributionsjl.readthedocs.io/en/latest/multivariate.html#multivariate-normal-distribution) package using following data.  \n",
    "    ```julia\n",
    "    using BenchmarkTools, Distributions\n",
    "\n",
    "    srand(280)\n",
    "    n, q = 2000, 10\n",
    "    Z = randn(n, q)\n",
    "    Ïƒ0, Ïƒ1 = 0.5, 2.0\n",
    "    Î£ = Ïƒ1^2 * Z * Z.' + Ïƒ0^2 * I\n",
    "    mvn = MvNormal(Î£) # MVN(0, Î£)\n",
    "    y = rand(mvn) # generate one instance from MNV(0, Î£)\n",
    "\n",
    "    # check you answer matches that from Distributions.jl\n",
    "    @show logpdf_mvn(y, Z, Ïƒ0, Ïƒ1)\n",
    "    @show logpdf(mvn, y)\n",
    "\n",
    "    # benchmark\n",
    "    @benchmark logpdf_mvn(y, Z, Ïƒ0, Ïƒ1)\n",
    "    @benchmark logpdf(mvn, y)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function logpdf_mvn{T <: AbstractFloat}(\n",
    "        y::Vector{T}, \n",
    "        Z::Matrix{T}, \n",
    "        Ïƒ0::T, \n",
    "        Ïƒ1::T\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "87px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
