{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biostat M280 Homework 5\n",
    "\n",
    "**Due June 15 @ 11:59PM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider again the MLE of the Dirichlet-multinomial model. In [HW4](http://hua-zhou.github.io/teaching/biostatm280-2018spring/hw/hw4/hw04.html), we worked out a Newton's method. In this homework, we explore the MM and EM approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1\n",
    "\n",
    "Show that, given iid observations $\\mathbf{x}_1,\\ldots,\\mathbf{x}_n$, the log-likelihood can be written as\n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\n",
    "+\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k).\n",
    "$$\n",
    "Hint: $\\Gamma(a + k) / \\Gamma(a) = a (a + 1) \\cdots (a+k-1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the hw4 and the hint shown above:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(\\alpha) &= \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d [\\ln \\Gamma(\\alpha_j + x_{ij}) - \\ln \\Gamma(\\alpha_j)] - \\sum_{i=1}^n [\\ln \\Gamma(|\\alpha|+|\\mathbf{x}_i|) - \\ln \\Gamma(|\\alpha|)] \\\\\n",
    "&= \\sum_{i=1}^n\\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d \\ln\\frac{\\Gamma(\\alpha_j + x_{ij})}{\\Gamma(\\alpha_j)} - \\sum_{i=1}^n \\ln\\frac{\\Gamma(|\\alpha|+|\\mathbf{x}_i|) }{\\Gamma(|\\alpha|)} \\\\\n",
    "&= \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d [\\ln (\\alpha_j(\\alpha_j+1)\\dots(\\alpha_j+x_{ij}-1)] - \\sum_{i=1}^n [\\ln |\\alpha|(|\\alpha|+1)\\dots(|\\alpha|+|\\mathbf{x}_i|-1)] \\\\\n",
    "& = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} +\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Hence the proof."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2\n",
    "\n",
    "Suppose $(P_1,\\ldots,P_d) \\in \\Delta_d = \\{\\mathbf{p}: p_i \\ge 0, \\sum_i p_i = 1\\}$ follows a Dirichlet distribution with parameter $\\alpha = (\\alpha_1,\\ldots,\\alpha_d)$. Show that\n",
    "$$\n",
    "\t\\mathbf{E}(\\ln P_j) = \\Psi(\\alpha_j) - \\Psi(|\\alpha|),\n",
    "$$\n",
    "where $\\Psi(z) = \\Gamma'(z)/\\Gamma(z)$ is the digamma function and $|\\alpha| = \\sum_{j=1}^d \\alpha_j$. Hint: Differentiate the identity \n",
    "$$\n",
    "1 = \\int_{\\Delta_d} \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the hint differentiate the identity: $1 = \\int_{\\Delta_d} \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}.$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "0 &= \\int_{\\Delta_d} \\frac{\\Gamma'(|\\alpha|)\\prod_{i=1}^d \\Gamma(\\alpha_i)-\\Gamma(|\\alpha|)\\Gamma'(\\alpha_j) \\prod_{i\\ne j}^d \\Gamma(\\alpha_i)}{(\\prod_{i=1}^d \\Gamma(\\alpha_i))^2}\\prod_{i=1}^d p_i^{\\alpha_i-1} + \\frac{\\Gamma(|\\alpha|)}{\\prod_{i=1}^d \\Gamma(\\alpha_i)} \\ln(p_j)\\prod_{i=1}^d p_i^{\\alpha_i-1} \\, d\\mathbf{p} \\\\\n",
    "&= \\frac{\\Gamma'(|\\alpha|)\\prod_{j=1}^d \\Gamma(\\alpha_j)- \\Gamma(|\\alpha|)\\left[\\prod_{j=1}^d \\Gamma(\\alpha_j)\\right]'}{\\left[\\prod_{j=1}^d \\Gamma(\\alpha_j)\\right]^2} * \\int_{\\Delta_d} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p} + \\left[\\int_{\\Delta_d} \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\ln(p_j)\\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}\\right] \\\\\n",
    "&= \\frac{\\Gamma'(|\\alpha|)\\prod_{j=1}^d \\Gamma(\\alpha_j)- \\Gamma(|\\alpha|)\\left[\\prod_{j=1}^d \\Gamma(\\alpha_j)\\right]'}{\\left[\\prod_{j=1}^d \\Gamma(\\alpha_j)\\right]^2} * \\int_{\\Delta_d} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p} + \\mathbf{E}(\\ln P_j) \\\\\n",
    "&= \\frac{\\Gamma'(|\\alpha|)\\prod_{j=1}^d \\Gamma(\\alpha_j)- \\Gamma(|\\alpha|)\\left[\\prod_{j=1}^d \\Gamma(\\alpha_j)\\right]'}{\\left[\\prod_{j=1}^d \\Gamma(\\alpha_j)\\right]^2} * \\frac{\\prod_{i=1}^d \\Gamma(\\alpha_i)}{\\Gamma(|\\alpha|)} + \\mathbf{E}(\\ln P_j) \\\\\n",
    "&= \\frac{\\Gamma'(|\\alpha|)}{\\Gamma(|\\alpha|)}  - \\frac{\\left[\\prod_{j=1}^d \\Gamma(\\alpha_j)\\right]'}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}+ \\mathbf{E}(\\ln P_j) \\\\\n",
    "&= \\frac{\\Gamma'(|\\alpha|)}{\\Gamma(|\\alpha|)}  - \\frac{1}{ \\Gamma(\\alpha_j)}+ \\mathbf{E}(\\ln P_j) \\\\\n",
    "&= \\Psi(|\\alpha|) -\\Psi(\\alpha_j)  + \\mathbf{E}(\\ln P_j) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Therefore, rearrange the above to give $\\mathbf{E}(\\ln P_j) = \\Psi(\\alpha_j) - \\Psi(|\\alpha|)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3\n",
    "\n",
    "The admixture representation of the Dirichlet-multinomial distribution suggests that we can treat the unobserved multinomial parameters $\\mathbf{p}_1,\\ldots,\\mathbf{p}_n$ as missing data and derive an EM algorithm. Show that the Q function is\n",
    "$$\n",
    "    Q(\\alpha|\\alpha^{(t)}) = \n",
    "\\sum_{j=1}^d \\sum_{i=1}^n \\alpha_j \\left[\\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|)\\right] - n \\sum_{j=1}^d \\ln \\Gamma(\\alpha_j) + n \\ln \\Gamma(|\\alpha|) + c^{(t)},\n",
    "$$\n",
    "where $c^{(t)}$ is a constant irrelevant to optimization. Comment on why it is not easy to maximize the Q function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Q_i (\\alpha|\\alpha^{(t)}) = \\mathbf{E}(\\prod _{i=1}^nf(\\mathbf{p}_i,\\mathbf{x}_i, \\mathbf{\\alpha}_i))$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q_i(\\alpha|\\alpha^{(t)}) \n",
    "&=\\int_{\\Delta_d}f(\\mathbf{p}_1,\\ldots,\\mathbf{p}_n |x_{1j} + \\alpha_j^{(t)}, \\ldots, x_{nj} + \\alpha_j^{(t)})\\ln \\left[f(\\mathbf{x}_i|\\mathbf{p}_i)\\pi(\\mathbf{p}_i)  \\right]\\, d\\mathbf{p}_i \\\\\n",
    "&= \\int_{\\Delta_d} \\frac{\\Gamma(|\\mathbf{x}_i| + |\\alpha^{(t)}|)}{\\prod_{j=1}^d \\Gamma(x_{ij} + \\alpha_j^{(t)})} \\prod_{j=1}^d p_{ij}^{x_{ij} + \\alpha_j^{(t)}-1}\\ln\\left[ \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\\prod_{j=1}^d p_{ij}^{x_j} \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j^{(t)})} \\prod_{j=1}^d p_{ij}^{\\alpha_j-1} \\right] \\, d\\mathbf{p}_i \\\\\n",
    "&= \\int_{\\Delta_d} \\frac{\\Gamma(|\\mathbf{x}_i| + |\\alpha^{(t)}|)}{\\prod_{j=1}^d \\Gamma(x_{ij} + \\alpha_j^{(t)})} \\prod_{j=1}^d p_{ij}^{x_{ij} + \\alpha_j^{(t)}-1} \\left[\\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}+\\sum_{j = 1}^d(x_{ij} +\\alpha_j -1)\\ln p_{ij}+\\ln\\Gamma(|\\alpha|) -\\sum_{j = 1}^d \\ln \\Gamma(\\alpha_j) \\right] \\, d\\mathbf{p}_i \\\\ \n",
    "&= \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} +\\sum_{j = 1}^d(x_{ij} +\\alpha_j -1) \\int_{\\Delta_d} \\left[ \\frac{\\Gamma(|\\mathbf{x}_i| + |\\alpha^{(t)}|)}{\\prod_{j=1}^d \\Gamma(x_{ij}  + \\alpha_j^{(t)})} \\prod_{j=1}^d p_{ij}^{x_{ij} + \\alpha_j^{(t)}-1}\\ln p_{ij}\\right] \\, d\\mathbf{p}_i + \\ln\\Gamma(|\\alpha|) -\\sum_{j = 1}^d \\ln \\Gamma(\\alpha_j) \\\\\n",
    "&= \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} +\\sum_{j = 1}^d(x_{ij} +\\alpha_j -1)\\mathbf{E}(\\ln P_{ij})+ \\ln\\Gamma(|\\alpha|) -\\sum_{j = 1}^d \\ln \\Gamma(\\alpha_j) \\\\\n",
    "\\text{By the result from Q2}\\\\\n",
    "&= \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} +\\sum_{j = 1}^d(x_{ij} +\\alpha_j -1)\\left[\\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|) \\right]+ \\ln\\Gamma(|\\alpha|) -\\sum_{j = 1}^d \\ln \\Gamma(\\alpha_j)\n",
    "\\end{aligned}\n",
    "$$\n",
    "Therefore, \n",
    "$$\n",
    "Q(\\alpha|\\alpha^{(t)}) = \\sum_{i =1}^n \\left[\\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} +\\sum_{j = 1}^d(x_{ij} +\\alpha_j -1)\\left[\\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|) \\right]+ \\ln\\Gamma(|\\alpha|) -\\sum_{j = 1}^d \\ln \\Gamma(\\alpha_j)\\right]\n",
    "$$\n",
    "$$\n",
    "= \\sum_{j=1}^d \\sum_{i=1}^n \\alpha_j \\left[\\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|)\\right] - n \\sum_{j=1}^d \\ln \\Gamma(\\alpha_j) + n \\ln \\Gamma(|\\alpha|) + c^{(t)},\n",
    "$$\n",
    "Hence the proof."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4\n",
    "\n",
    "We derive an MM algorithm for maximing $L$. Consider the formulation of the log-likelihood that contains terms $\\ln (\\alpha_j + k)$ and $- \\ln (|\\alpha|+k)$. Applying Jensen's inequality to the concave term $\\ln (\\alpha_j + k)$ and supporting hyperplane inequality to the convex term $- \\ln (|\\alpha|+k)$, show that a minorizing function to $L(\\alpha)$ is\n",
    "$$\n",
    "\tg(\\alpha|\\alpha^{(t)}) = - \\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k} |\\alpha| + \\sum_{j=1}^d \\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk} \\alpha_j^{(t)}}{\\alpha_j^{(t)}+k} \\ln \\alpha_j + c^{(t)},\n",
    "$$\n",
    "where $s_{jk} = \\sum_{i=1}^n 1_{\\{x_{ij} > k\\}}$, $r_k = \\sum_{i=1}^n 1_{\\{|\\mathbf{x}_i| > k\\}}$, and  $c^{(t)}$ is a constant irrelevant to optimization. Maximizing the surrogate function $g(\\alpha|\\alpha^{(t)})$ is trivial since $\\alpha_j$ are separated. Show that the MM updates are\n",
    "$$\n",
    "\t\\alpha_j^{(t+1)} = \\frac{\\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk}}{\\alpha_j^{(t)}+k}}{\\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k}} \\alpha_j^{(t)}, \\quad j=1,\\ldots,d.\n",
    "$$\n",
    "The quantities $s_{jk}$, $r_k$, $\\max_i x_{ij}$ and $\\max_i |\\mathbf{x}_i|$ only depend on data and can be pre-computed. Comment on whether the MM updates respect the parameter constraint $\\alpha_j>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the Question 1:\n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\n",
    "+\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k).\n",
    "$$\n",
    "\n",
    "Define $s_{jk} = \\sum_{i=1}^n 1_{\\{x_{ij} > k\\}}$, $r_k = \\sum_{i=1}^n 1_{\\{|\\mathbf{x}_i| > k\\}}$, we have \n",
    "\n",
    "$$\n",
    "L(\\alpha) =  \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) s_{jk}- \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k)r_k + c^{(t)}\n",
    "$$\n",
    "\n",
    "where $c^{(t)}$ is a constant irrelevant to optimization. \n",
    "\n",
    "Applying Jensen's inequality to compute the lower bound of $\\ln(\\alpha_j+k)$,\n",
    "\n",
    "$$\\ln(\\alpha_j+k) \\ge \\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)}+k}\\ln\\bigg(\\frac{\\alpha_j^{(t)}+k}{\\alpha_j^{(t)}}\\cdot \\alpha_j\\bigg)+\\frac{k}{\\alpha_j^{(t)}+k}\\ln\\bigg(\\frac{\\alpha_j^{(t)}+k}{k}\\cdot k\\bigg)=\\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)}+k}\\ln(\\alpha_j)+c^{(t)}$$\n",
    "\n",
    "and the hyperplane inequality to compute the upperbound of $\\ln(|\\alpha|+k)$,\n",
    "\n",
    "$$\\ln(|\\alpha|+k)\\le \\frac{|\\alpha|-|\\alpha^{(t)}|}{|\\alpha^{(t)}|+k}+\\ln(|\\alpha|+k)=\n",
    "\\frac{|\\alpha|-|\\alpha^{(t)}|}{|\\alpha^{(t)}|+k}.$$\n",
    "\n",
    "Then the minorizing fuction is given by:\n",
    "\n",
    "$$\n",
    "L(\\alpha) =  \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) s_{jk}- \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k)r_k + c^{(t)}\\ge \n",
    "\\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)}+k}\\ln(\\alpha_j) s_{jk}- \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\frac{|\\alpha|-|\\alpha^{(t)}|}{|\\alpha^{(t)}|+k}r_k + c^{(t)}=g(\\alpha|\\alpha^{(t)}).\n",
    "$$\n",
    "\n",
    "\n",
    "Based on the minorization function, the MM update can be computed by partial derivative on $\\alpha_j$: \n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\alpha_j}g(\\alpha|\\alpha^{(t)})= - \\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k} +\\frac{1}{\\alpha_j}\\cdot\\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk} \\alpha_j^{(t)}}{\\alpha_j^{(t)}+k}=0$$\n",
    "\n",
    "==> \n",
    "$$\\alpha_j^{(t+1)} = \\frac{\\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk} \\alpha_j^{(t)}}{\\alpha_j^{(t)}+k}}{\\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k}} \\quad j=1,\\ldots,d.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  550k  100  550k    0     0  1242k      0 --:--:-- --:--:-- --:--:-- 1242k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  258k  100  258k    0     0   583k      0 --:--:-- --:--:-- --:--:--  583k\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "64×3823 Array{Int64,2}:\n",
       "  0   0   0   0   0   0   0   0   0  …   0   0   0   0   0   0   0   0   0\n",
       "  1   0   0   0   0   0   0   0   0      0   0   1   0   0   0   0   0   0\n",
       "  6  10   8   0   5  11   1   8  15      9   9  10   6   5   0   3   6   2\n",
       " 15  16  15   3  14  16  11  10   2     16  16  16  16  13   1  15  16  15\n",
       " 12   6  16  11   4  10  13   8  14      6  12  16  11  11  12   0   2  16\n",
       "  1   0  13  16   0   1  11   7  13  …   0   1   4   0   2   1   0   0  13\n",
       "  0   0   0   0   0   0   7   2   2      0   0   0   0   0   0   0   0   1\n",
       "  0   0   0   0   0   0   0   0   0      0   0   0   0   0   0   0   0   0\n",
       "  0   0   0   0   0   0   0   0   0      0   0   0   0   0   0   0   0   0\n",
       "  7   7   1   0   0   4   0   1   0      2   3   8   1   2   0   0   0   0\n",
       " 16  16  11   5  13  16   9  15  16  …  14  16  13  16  15   0  11  15   3\n",
       "  6   8   9  16   8  10  14  14  15     16  10  12   2   6  14  14  10   7\n",
       "  6  16  11  11   0  15   6  12  12     16  15  16  12   5  10   0   0  10\n",
       "  ⋮                   ⋮              ⋱       ⋮                   ⋮        \n",
       " 15  10   4  16   4  15  15  14  16      8   4  10   0   1  15   9   2   7\n",
       "  9  15   0   0  11   8   3   5   6      0   9  16   8   8   1  15   8   0\n",
       "  0   3   0   0  12   8   0   0   0      0  16   5   4  10   0   4  15   0\n",
       "  0   0   0   0   0   3   0   0   0  …   0   0   0   0   0   0   0   0   0\n",
       "  0   0   0   0   0   0   0   0   0      0   0   0   0   0   0   0   0   0\n",
       "  0   0   0   0   0   0   0   0   0      0   0   2   1   0   0   0   0   0\n",
       "  6  10   9   0   4  10   1   4  10     10   8  13   7   8   0   4   5   4\n",
       " 14  16  14   1  12  16  13  13  12     13  16  16  14  13   4  14  16  14\n",
       "  7  15   0  15  14  16   5   8   5  …   1  16  12  16  15   9  16  16   1\n",
       "  1   3   0   2   7  16   0   0   0      0  16   5  12  10   0   9  16   0\n",
       "  0   0   0   0   0  16   0   0   0      0   8   0   1   1   0   0   5   0\n",
       "  0   0   0   0   0   6   0   0   0      0   0   0   0   0   0   0   0   0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download file if it's not in current folder\n",
    "download(\"http://hua-zhou.github.io/teaching/\" * \n",
    "    \"biostatm280-2018spring/hw/hw4/optdigits.tra\", \n",
    "    \"optdigits.tra\")\n",
    "\n",
    "download(\"http://hua-zhou.github.io/teaching/\" * \n",
    "    \"biostatm280-2018spring/hw/hw4/optdigits.tes\",\n",
    "    \"optdigits.tes\")\n",
    "\n",
    "data1 = readcsv(\"optdigits.tra\", Int)\n",
    "label = data1[:, end]\n",
    "features = data1[:, 1:64]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5\n",
    "\n",
    "Write a function for finding MLE of Dirichlet-multinomial distribution given iid observations $\\mathbf{x}_1,\\ldots,\\mathbf{x}_n$, using MM algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First copy the functions from HW4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_logpdf"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dirmult_logpdf(x::Vector, α::Vector)\n",
    "    \n",
    "Compute the log-pdf of Dirichlet-multinomial distribution with parameter `α` \n",
    "at data point `x`.\n",
    "\"\"\"\n",
    "function dirmult_logpdf(x::AbstractVector, α::Vector)\n",
    "    n = size(x)[1]\n",
    "    d = size(α)\n",
    "\n",
    "    l = 0.0\n",
    "    \n",
    "    # These parts are computed from each element of x\n",
    "    # ∑[-ln(x_ij) + ln(Γ(α_j + x_{ij})) - ln(Γ(α_j))]\n",
    "    for i = 1:n\n",
    "        if α[i] == 0 && x[i] > 0\n",
    "            return -Inf\n",
    "        elseif α[i] > 0\n",
    "            l += - lfact(x[i]) - lgamma(α[i]) + lgamma(x[i] + α[i])\n",
    "        end    \n",
    "    end\n",
    "\n",
    "    # These parts are unrelated to each element of x\n",
    "    # ln(|x|) - ln(Γ(|α| + |x|)) + ln(Γ(|α|))\n",
    "    norm_x = sum(x)\n",
    "    norm_α = sum(α)   \n",
    "    if norm_x == 0 && norm_α == 0\n",
    "        return 0\n",
    "    elseif norm_x > 0 && norm_α == 0\n",
    "        return -Inf\n",
    "    else\n",
    "        l += lfact(norm_x) - lgamma(norm_α + norm_x) + lgamma(norm_α)\n",
    "    end\n",
    "    return l\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    dirmult_logpdf(X, α)\n",
    "    \n",
    "Compute the log-pdf of Dirichlet-multinomial distribution with parameter `α` \n",
    "at each data point in `X`. Each column of `X` is one data point.\n",
    "\"\"\"\n",
    "function dirmult_logpdf(X::AbstractMatrix, α::Vector)\n",
    "    r = 0\n",
    "    for j in 1:size(X, 2)\n",
    "        r += dirmult_logpdf(view(X, :, j), α)\n",
    "    end\n",
    "    return r\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-478546.7203005796"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirmult_logpdf(features, dirmult_init_alpha(features)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_init_alpha (generic function with 2 methods)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the method of moment estimator to initialize α_j\n",
    "\n",
    "function dirmult_init_alpha(X::AbstractMatrix)\n",
    "   # n is the number of samples, d is the number of parameters\n",
    "    X = X'\n",
    "    n = size(X, 1)\n",
    "    d = size(X, 2)\n",
    "   # First compute the first and second moment of X_i\n",
    "    first_moment = zeros(n, d)\n",
    "    second_moment = zeros(n, d)\n",
    "   \n",
    "   # Need to handle the case when sum(X, 2) = 0 \n",
    "    first_moment = X ./ ((1e-32 + sum(X, 2)) * ones(1, d))\n",
    "    second_moment = first_moment .^ 2\n",
    "   \n",
    "   # Then estimate the w, need to handle the case when sum(first_moment, 1) = 0\n",
    "    w = sum(sum(second_moment, 1) ./ (1e-32 + sum(first_moment, 1)))\n",
    "   \n",
    "   # Compute the α\n",
    "    α = ((d - w) / (w - 1)) .* sum(X, 1) ./ sum(sum(X, 1))\n",
    "    return vec(α)\n",
    "end  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64×1 Array{Float64,2}:\n",
       " 0.0        \n",
       " 0.0718341  \n",
       " 1.3068     \n",
       " 2.81438    \n",
       " 2.72988    \n",
       " 1.31241    \n",
       " 0.330736   \n",
       " 0.0339217  \n",
       " 0.000498848\n",
       " 0.467358   \n",
       " 2.52149    \n",
       " 2.7928     \n",
       " 2.53284    \n",
       " ⋮          \n",
       " 2.32781    \n",
       " 2.21302    \n",
       " 0.892502   \n",
       " 0.0353559  \n",
       " 6.2356e-5  \n",
       " 0.0674692  \n",
       " 1.39596    \n",
       " 2.84705    \n",
       " 2.73219    \n",
       " 1.59731    \n",
       " 0.501966   \n",
       " 0.0482012  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "α=dirmult_init_alpha(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_gradient (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using SpecialFunctions\n",
    "function dirmult_gradient(X::AbstractMatrix, α::Array)\n",
    "    X = X'\n",
    "    n, d = size(X)\n",
    "    sum_α = sum(α)\n",
    "    sum_x = sum(X, 2)\n",
    "        \n",
    "    ∇ = zeros(d)\n",
    "    \n",
    "    # First compute \\sum_{i=1}^n [ψ(|\\mathbf{x}_i|+|α|) - ψ(|α|)] as it holds for every j\n",
    "    const_j = 0\n",
    "    for i in 1:n\n",
    "        const_j += (digamma(sum_x[i] + sum_α))\n",
    "    end\n",
    "    \n",
    "    const_j -= (n * digamma(sum_α))\n",
    "\n",
    "    # Then compute \\sum_{i=1}^n [ψ(x_{ij}+α_j) - ψ(α_j)]\n",
    "    for j in 1:d\n",
    "        for i in 1:n\n",
    "            if α[j] == 0 && X[i, j] > 0\n",
    "                ∇[j] += Inf\n",
    "            elseif α[j] > 0\n",
    "                ∇[j] += digamma(X[i, j] + α[j]) - digamma(α[j])\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        # Finally combine with the const part\n",
    "        ∇[j] -= const_j\n",
    "    end\n",
    "    \n",
    "    return ∇\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64-element Array{Float64,1}:\n",
       " -6303.55  \n",
       "  2330.32  \n",
       "  -161.083 \n",
       "   282.381 \n",
       "   243.219 \n",
       "  -700.941 \n",
       " -2213.62  \n",
       " -2770.39  \n",
       "  -286.619 \n",
       "  -723.527 \n",
       "    25.9761\n",
       "   366.041 \n",
       "   224.47  \n",
       "     ⋮     \n",
       "    91.2016\n",
       "  -256.095 \n",
       " -1194.64  \n",
       "  -318.957 \n",
       "  9733.39  \n",
       "  1180.57  \n",
       "  -272.575 \n",
       "   262.399 \n",
       "   121.128 \n",
       "  -553.934 \n",
       " -1863.97  \n",
       " -1757.43  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirmult_gradient(features, α)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_obs (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observed information matrix\n",
    "#Return vector d and constant `c`. The observed information matrix equals `Diagonal(d) - c`.\n",
    "function dirmult_obs(X::AbstractMatrix, α::Array)\n",
    "    X = X'\n",
    "    n, r = size(X)\n",
    "    sum_α = sum(α)\n",
    "    sum_x = sum(X, 2)\n",
    "    \n",
    "    # First compute \\sum_{i=1}^n [ψ'(|α|) - ψ'(|\\mathbf{x}_i|+|α|) ] as it holds for every j\n",
    "    c = 0\n",
    "        \n",
    "    for i in 1:n\n",
    "        if sum_α == 0 && sum_x > 0\n",
    "            c += Inf\n",
    "        elseif sum_α > 0\n",
    "            c += trigamma(sum_α) - (trigamma(sum_x[i] + sum_α))\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Then compute \\sum_{i=1}^n [ψ'(α_j) - ψ'(x_{ij}+α_j)]\n",
    "    d = zeros(r)\n",
    "    for j in 1:r\n",
    "        for i in 1:n\n",
    "            if α[j] == 0 && X[i, j] > 0\n",
    "                d[j] += Inf\n",
    "            elseif α[j] > 0\n",
    "                d[j] += trigamma(α[j]) - trigamma(X[i, j] + α[j])\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return c, d\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50.0163869748493, [0.0, 688.915, 4313.97, 5730.92, 5665.9, 3860.62, 1242.08, 151.565, 3.71361, 2231.94  …  2853.36, 255.11, 1.0, 574.829, 4306.32, 5688.55, 5469.75, 4162.56, 1787.01, 264.045])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirmult_obs(features, ones(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_mm"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dirmult_mm(X)\n",
    "\n",
    "Find the MLE of Dirichlet-multinomial distribution using MM algorithm.\n",
    "\n",
    "# Argument\n",
    "* `X`: an `d`-by-`n` matrix of counts; each column is one data point.\n",
    "\n",
    "# Optional argument  \n",
    "* `alpha0`: starting point. \n",
    "* `maxiters`: the maximum allowable Newton iterations (default 100). \n",
    "* `tolfun`: the tolerance for  relative change in objective values (default 1e-6). \n",
    "\n",
    "# Output\n",
    "# Output\n",
    "* `logl`: the log-likelihood at MLE.   \n",
    "* `niter`: the number of iterations performed.\n",
    "# `α`: the MLE.\n",
    "* `∇`: the gradient at MLE. \n",
    "* `obsinfo`: the observed information matrix at MLE. \n",
    "\"\"\"\n",
    "function dirmult_mm(\n",
    "        X::AbstractMatrix; \n",
    "        α0::Vector = dirmult_init_alpha(X)[:, 1],\n",
    "        maxiters::Int = 100, \n",
    "        tolfun = 1e-8,\n",
    "        debug::Bool = false\n",
    "    )\n",
    "    \n",
    "    # Transform the fat X to tall as it is consistent\n",
    "    # with the formular\n",
    "    X = X'\n",
    "    \n",
    "    # filter out zero rows and columns\n",
    "    index_y, index_x = find(sum(X, 1)), find(sum(X, 2))\n",
    "    X, α = X[index_x, index_y], α0[index_y]\n",
    "\n",
    "    # Initialize variables\n",
    "    α2 = similar(α)\n",
    "    n, d = size(X)\n",
    "    iters = 0\n",
    "    \n",
    "    # Compute the initial log pdf\n",
    "    lp_old = dirmult_logpdf(X', α)  \n",
    "    if debug\n",
    "        println(\"iteration 0\", \", logl = \", lp_old)\n",
    "    end\n",
    "    \n",
    "    # Prepare important variables \n",
    "    rowsums = sum(X, 2)\n",
    "    logliter = 0.0\n",
    "    colmax = maximum(X, 1)\n",
    "\n",
    "    for iter in 1:maxiters\n",
    "        # first calculate the denominator\n",
    "        denom = 0\n",
    "        for k in 0:(maximum(rowsums) - 1)\n",
    "            r = sum(rowsums .> k)\n",
    "            denom = denom + r / (sum(α) + k)\n",
    "        end\n",
    "        \n",
    "        # then calculate the numerator\n",
    "        for j in 1:d\n",
    "            tempt = 0\n",
    "            if(colmax[j] >= 1)\n",
    "                for k in 0:(colmax[j] - 1)\n",
    "                    s = sum(X[:, j] .> k)\n",
    "                    tempt = tempt + s / (α[j] + k)\n",
    "                end\n",
    "                α2[j] = tempt / denom * α[j]\n",
    "            end\n",
    "        end\n",
    "        logliter = dirmult_logpdf(X', α2)\n",
    "\n",
    "        # debug \n",
    "        if debug\n",
    "            println(\"iteration \", iter, \", logl = \", logliter)\n",
    "        end\n",
    "        \n",
    "        α = α2\n",
    "        if abs(logliter - lp_old) < tolfun * (abs(lp_old) + 1)\n",
    "            iters = iter\n",
    "            break\n",
    "        end\n",
    "        lp_old = logliter\n",
    "        iters = maxiters\n",
    "    end\n",
    "\n",
    "    # compute the final results\n",
    "    αfinal = zeros(eltype(α0), length(α0))\n",
    "    αfinal[index_y] = α\n",
    "    \n",
    "    ∇final = zeros(eltype(α0), length(α0))\n",
    "    ∇final[index_y] = dirmult_gradient(X', α)\n",
    "\n",
    "    obs = zeros(eltype(α0), length(α0), length(α0))\n",
    "    c, d = dirmult_obs(X', α)\n",
    "    obs[index_y, index_y] = diagm(d) - c\n",
    "     \n",
    "    return logliter, iters, αfinal, ∇final, obs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, logl = -478546.7203005798\n",
      "iteration 1, logl = -475812.5993711112\n",
      "iteration 2, logl = -474921.7764539261\n",
      "iteration 3, logl = -474362.62958675495\n",
      "iteration 4, logl = -473928.3914190551\n",
      "iteration 5, logl = -473575.3571699772\n",
      "iteration 6, logl = -473286.45841890486\n",
      "iteration 7, logl = -473050.47069795476\n",
      "iteration 8, logl = -472858.3671056909\n",
      "iteration 9, logl = -472702.54746868776\n",
      "iteration 10, logl = -472576.5864036902\n",
      "iteration 11, logl = -472475.0794899885\n",
      "iteration 12, logl = -472393.5115004179\n",
      "iteration 13, logl = -472328.1348225659\n",
      "iteration 14, logl = -472275.8577233817\n",
      "iteration 15, logl = -472234.1434885889\n",
      "iteration 16, logl = -472200.9210143533\n",
      "iteration 17, logl = -472174.506803252\n",
      "iteration 18, logl = -472153.53783295344\n",
      "iteration 19, logl = -472136.9144542686\n",
      "iteration 20, logl = -472123.7523070059\n",
      "iteration 21, logl = -472113.342180847\n",
      "iteration 22, logl = -472105.11676115973\n",
      "iteration 23, logl = -472098.6232600973\n",
      "iteration 24, logl = -472093.50102105807\n",
      "iteration 25, logl = -472089.46328519203\n",
      "iteration 26, logl = -472086.2824123074\n",
      "iteration 27, logl = -472083.7779483758\n",
      "iteration 28, logl = -472081.8070244252\n",
      "iteration 29, logl = -472080.25665468944\n",
      "iteration 30, logl = -472079.0375747696\n",
      "iteration 31, logl = -472078.079323497\n",
      "iteration 32, logl = -472077.32632561825\n",
      "iteration 33, logl = -472076.73477738537\n",
      "iteration 34, logl = -472076.2701744799\n",
      "iteration 35, logl = -472075.9053526261\n",
      "iteration 36, logl = -472075.6189365152\n",
      "iteration 37, logl = -472075.39411336184\n",
      "iteration 38, logl = -472075.2176640824\n",
      "iteration 39, logl = -472075.079198651\n",
      "iteration 40, logl = -472074.9705530851\n",
      "iteration 41, logl = -472074.88531423773\n",
      "iteration 42, logl = -472074.8184454948\n",
      "iteration 43, logl = -472074.76599212043\n",
      "iteration 44, logl = -472074.7248494644\n",
      "iteration 45, logl = -472074.69258061814\n",
      "iteration 46, logl = -472074.66727307776\n",
      "iteration 47, logl = -472074.6474260881\n",
      "iteration 48, logl = -472074.6318621298\n",
      "iteration 49, logl = -472074.61965739384\n",
      "iteration 50, logl = -472074.61008718115\n",
      "iteration 51, logl = -472074.60258303414\n",
      "iteration 52, logl = -472074.5966990804\n",
      "iteration 53, logl = -472074.59208562266\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-472074.59208562266, 53, [0.0, 0.0806136, 0.889659, 2.08822, 2.00446, 0.759612, 0.145733, 0.0150841, 0.000388142, 0.299973  …  0.457102, 0.0271415, 0.000129329, 0.0651973, 0.914253, 2.09744, 1.92763, 0.938993, 0.230473, 0.0276847], [0.0, -0.402009, -0.694345, -0.819453, -0.816881, -0.697797, -0.473921, -0.391207, -0.3815, -0.524293  …  -0.635104, -0.393486, -0.381329, -0.402848, -0.711732, -0.825503, -0.834029, -0.75953, -0.522586, -0.395799], [0.0 0.0 … 0.0 0.0; 0.0 91478.4 … -69.3698 -69.3698; … ; 0.0 -69.3698 … 25449.6 -69.3698; 0.0 -69.3698 … -69.3698 2.70184e5])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirmult_mm(features, debug = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6\n",
    "\n",
    "Re-do [HW4 Q9](http://hua-zhou.github.io/teaching/biostatm280-2018spring/hw/hw4/hw04.html#Q9) using your new `dirmult_mm` function. Compare the number of iterations and run time by MM algorithm to those by Newton's method. Comment on the efficiency of Newton's algorithm vs MM algorithm for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 1.264659846 seconds\n",
      "elapsed time: 1.189961109 seconds\n",
      "elapsed time: 0.470328969 seconds\n",
      "elapsed time: 1.293357536 seconds\n",
      "elapsed time: 0.914878721 seconds\n",
      "elapsed time: 0.951083447 seconds\n",
      "elapsed time: 1.025873888 seconds\n",
      "elapsed time: 0.534164038 seconds\n",
      "elapsed time: 1.111876944 seconds\n",
      "elapsed time: 1.131002806 seconds\n"
     ]
    }
   ],
   "source": [
    "using DataFrames, Distributions\n",
    "\n",
    "# First initialize the variables\n",
    "digits  = zeros(Int, 10)\n",
    "log_lp, time_seconds = zeros(10), zeros(10)\n",
    "iters = zeros(Int, 10)\n",
    "α = zeros(64, 10)\n",
    "\n",
    "\n",
    "for n in 1:10\n",
    "    # retrieve data for digit d\n",
    "    X = data1[data1[:, end] .== (n-1), 1:64]\n",
    "    X = X'\n",
    "    digits[n] = size(X, 2)\n",
    "    # Apply Dirichlet-multinomial\n",
    "    tic()\n",
    "    log_lp[n], iters[n], α1 = dirmult_mm(X)\n",
    "    α[:, n] = α1\n",
    "    time_seconds[n] = toc()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10×5 DataFrames.DataFrame\n",
      "│ Row │ digit │ n   │ logl_dm  │ iters │ runtime  │\n",
      "├─────┼───────┼─────┼──────────┼───────┼──────────┤\n",
      "│ 1   │ 0     │ 376 │ -37361.9 │ 100   │ 1.14497  │\n",
      "│ 2   │ 1     │ 389 │ -42179.2 │ 82    │ 0.94027  │\n",
      "│ 3   │ 2     │ 380 │ -39985.3 │ 31    │ 0.349501 │\n",
      "│ 4   │ 3     │ 389 │ -40519.5 │ 100   │ 1.04529  │\n",
      "│ 5   │ 4     │ 387 │ -43488.8 │ 56    │ 0.59262  │\n",
      "│ 6   │ 5     │ 376 │ -41191.3 │ 70    │ 0.708143 │\n",
      "│ 7   │ 6     │ 377 │ -37702.5 │ 79    │ 0.749575 │\n",
      "│ 8   │ 7     │ 387 │ -40304.0 │ 38    │ 0.384146 │\n",
      "│ 9   │ 8     │ 380 │ -43130.9 │ 73    │ 0.763601 │\n",
      "│ 10  │ 9     │ 382 │ -43709.7 │ 72    │ 0.781177 │"
     ]
    }
   ],
   "source": [
    "results = DataFrame(digit = 0:9, count = digits, \n",
    "    logpdf = log_lp, iters = iters, runtime = time_seconds)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mWARNING: \u001b[39m\u001b[22m\u001b[33mlog(x::AbstractArray{T}) where T <: Number is deprecated, use log.(x) instead.\u001b[39m\n",
      "Stacktrace:\n",
      " [1] \u001b[1mdepwarn\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::Symbol\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./deprecated.jl:70\u001b[22m\u001b[22m\n",
      " [2] \u001b[1mlog\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Float64,1}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./deprecated.jl:57\u001b[22m\u001b[22m\n",
      " [3] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./loading.jl:522\u001b[22m\u001b[22m\n",
      " [4] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Module, ::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/huiyuhu/.julia/v0.6/Compat/src/Compat.jl:71\u001b[22m\u001b[22m\n",
      " [5] \u001b[1mexecute_request\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::ZMQ.Socket, ::IJulia.Msg\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/huiyuhu/.julia/v0.6/IJulia/src/execute_request.jl:158\u001b[22m\u001b[22m\n",
      " [6] \u001b[1m(::Compat.#inner#17{Array{Any,1},IJulia.#execute_request,Tuple{ZMQ.Socket,IJulia.Msg}})\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/huiyuhu/.julia/v0.6/Compat/src/Compat.jl:385\u001b[22m\u001b[22m\n",
      " [7] \u001b[1meventloop\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::ZMQ.Socket\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/huiyuhu/.julia/v0.6/IJulia/src/eventloop.jl:8\u001b[22m\u001b[22m\n",
      " [8] \u001b[1m(::IJulia.##14#17)\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./task.jl:335\u001b[22m\u001b[22m\n",
      "while loading In[93], in expression starting on line 15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10×10 Array{Int64,2}:\n",
       " 174    0    0    0    4    0    0    0    0    0\n",
       "   0  132   21    0    2    0    2    0   14   11\n",
       "   0    9  151    2    0    1    1    1    8    4\n",
       "   0    2    1  154    0    4    0    7    5   10\n",
       "   0    2    0    0  173    0    1    2    2    1\n",
       "   0    0    0    0    1  166    1    0    0   14\n",
       "   0    6    0    0    2    1  170    0    2    0\n",
       "   0    0    0    0   11    0    0  164    2    2\n",
       "   0   22    1    0    1    1    1    1  134   13\n",
       "   0    5    0    4    6    2    0    3    4  156"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using MLBase\n",
    "\n",
    "# This is a standard method to build the confusion matrix\n",
    "\n",
    "# load test digits\n",
    "testdata = readcsv(\"optdigits.tes\", Int)\n",
    "Xtest    = testdata[:, 1:64]'\n",
    "ytest    = testdata[:, 65]\n",
    "\n",
    "# matrix of assignment probability\n",
    "test_digit_prob = [dirmult_logpdf(Xtest[:, j], α[:, d]) \n",
    "    for d in 1:10, j in 1:size(Xtest, 2)]\n",
    "\n",
    "# factor in prior probabilities\n",
    "test_digit_prob .= test_digit_prob .+ log(digits / sum(digits))\n",
    "\n",
    "# prediction\n",
    "pred = [indmax(test_digit_prob[:, j]) for j in 1:size(Xtest, 2)]\n",
    "\n",
    "# confusion matrix\n",
    "confmat = confusmat(10, ytest + 1, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7\n",
    "\n",
    "Finally let us re-consider the EM algorithm. The difficulty with the M step in EM algorithm can be remedied. Discuss how we can further minorize the $\\ln \\Gamma(|\\alpha|)$ term in the $Q$ function to produce a minorizing function with all $\\alpha_j$ separated. For this homework, you do **not** need to implement this EM-MM hybrid algorithm. Hint: $z \\mapsto \\ln \\Gamma(z)$ is a convex function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the $z \\mapsto \\ln \\Gamma(z)$ is a convex function, \n",
    "$$\\ln\\Gamma(|\\alpha|) >= \\Psi(|\\alpha^{(t)}|)(|\\alpha|-|\\alpha^{(t)}|) + \\ln\\Gamma(|\\alpha^{(t)}|)$$\n",
    "Then,\n",
    "$$ Q(\\alpha|\\alpha^{(t)}) = \\sum_{j=1}^d \\sum_{i=1}^n \\alpha_j \\left[\\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|)\\right] - n \\sum_{j=1}^d \\ln \\Gamma(\\alpha_j) + n \\ln \\Gamma(|\\alpha|) + c^{(t)}$$\n",
    "$$ >= \\sum_{j=1}^d \\sum_{i=1}^n \\alpha_j \\left[\\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|)\\right] - n \\sum_{j=1}^d \\ln \\Gamma(\\alpha_j) + n \\ln \\Gamma(|\\alpha^{(t)}|) +n \\Psi(|\\alpha^{(t)}|) (|\\alpha| - |\\alpha^{(t)}|) + c^{(t)}$$\n",
    "\n",
    "Therefore, resulting in all $\\alpha_j$ and then we can calculate $\\alpha_j$ according to the last iteration of $\\alpha_j$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "65px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
